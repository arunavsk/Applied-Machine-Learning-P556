{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A3_P556_F19.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tg9fRSjFtsRu"
      },
      "source": [
        "# Assignment #3\n",
        "## P556: Applied Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jjsZpx5C9eBH"
      },
      "source": [
        "More often than not, we will use a deep learning library (Tensorflow, Pytorch, or the wrapper known as Keras) to implement our models. However, the abstraction afforded by those libraries can make it hard to troubleshoot issues if we don't understand what is going on under the hood. In this assignment you will implement a fully-connected and a convolutional neural network from scratch. To simplify the implementation, we are asking you to implement static architectures, but you are free to support variable number of layers/neurons/activations/optimizers/etc. We recommend that you make use of private methods so you can easily troubleshoot small parts of your model as you develop them, instead of trying to figure out which parts are not working correctly after implementing everything. Also, keep in mind that there is code from your fully-connected neural network that can be re-used on the CNN. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2NzW9M-btzqO"
      },
      "source": [
        "Problem #1.1 (40 points): Implement a fully-connected neural network from scratch. The neural network will have the following architecture:\n",
        "\n",
        "- Input layer\n",
        "- Dense hidden layer with 512 neurons, using relu as the activation function\n",
        "- Dropout with a value of 0.2\n",
        "- Dense hidden layer with 512 neurons, using relu as the activation function\n",
        "- Dropout with a value of 0.2\n",
        "- Output layer, using softmax as the activation function\n",
        "\n",
        "The model will use categorical crossentropy as its loss function. \n",
        "We will optimize the gradient descent using RMSProp, with a learning rate of 0.001 and a rho value of 0.9.\n",
        "We will evaluate the model using accuracy.\n",
        "\n",
        "Why this architecture? We are trying to reproduce from scratch the following [example from the Keras documentation](https://keras.io/examples/mnist_mlp/). This means that you can compare your results by running the Keras code provided above to see if you are on the right track."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKLtOiqRbFkx",
        "colab_type": "text"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk6pC2j6JMO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "import time\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDb1w0sYrIKe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Linear(object):\n",
        "  def __init__(self,input_nodes, output_nodes, activation, dropout):\n",
        "    \n",
        "    self.input_nodes = input_nodes\n",
        "    self.output_nodes = output_nodes\n",
        "    self.activation = activation\n",
        "    self.W = np.random.rand(self.input_nodes, self.output_nodes) * np.sqrt(2/self.input_nodes) # He initialization\n",
        "    self.B = np.zeros(( 1, self.output_nodes)) \n",
        "    self.s_W = np.zeros((self.input_nodes, self.output_nodes))\n",
        "    self.s_B = np.zeros((1, self.output_nodes))\n",
        "    self.dropout = dropout\n",
        "\n",
        "  def layer(self, X):\n",
        "\n",
        "    self.X = X\n",
        "    self.Z = np.dot(self.X, self.W) + self.B\n",
        "    self.A = self.activation(self.Z)\n",
        "    # return W, Z, A\n",
        "  \n",
        "  def compute_drop(self):\n",
        "\n",
        "    ## Dropout implementation from here - https://rohanvarma.me/Neural-Net/\n",
        "\n",
        "    self.keep_p = 1-self.dropout\n",
        "    mult = np.random.binomial(1, self.keep_p, size = (1, self.output_nodes))\n",
        "    self.A = self.A/self.keep_p\n",
        "    self.A *= mult\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8rPUmRqBtpS2",
        "colab": {}
      },
      "source": [
        "class NeuralNetwork(object):\n",
        "  def __init__(self, epochs, learning_rate, dropout, rho):\n",
        "    # network parameters go here\n",
        "    self.epochs = epochs\n",
        "    self.learning_rate = learning_rate\n",
        "    self.rho = rho\n",
        "    \n",
        "    # define the network here\n",
        "    self.l1 = Linear(784, 512, ReLU, dropout)\n",
        "    self.l2 = Linear(512, 512, ReLU, dropout)\n",
        "    self.l3 = Linear(512, 10, softmax, 0)\n",
        "  \n",
        "  def forward_pass(self, X_test = np.array([None])):\n",
        "    if X_test.all() == None:\n",
        "      input_X = self.X\n",
        "    else:\n",
        "      input_X = X_test\n",
        "    self.l1.layer(input_X)\n",
        "    self.l1.compute_drop()\n",
        "    self.l2.layer(self.l1.A)\n",
        "    self.l2.compute_drop()\n",
        "    self.l3.layer(self.l2.A)\n",
        "    self.output = self.l3.A\n",
        "\n",
        "  def correct_predictions(self, actuals = np.array([None])):\n",
        "    if actuals.all() == None:\n",
        "      actuals = self.y\n",
        "    correct_pred = sum(np.argmax(self.l3.A, axis=1) == np.argmax(actuals, axis = 1))\n",
        "    return correct_pred\n",
        "\n",
        "  def calculate_loss(self):\n",
        "    log_likelihood = -self.y* np.log(self.output)\n",
        "    loss = np.sum(log_likelihood)/len(self.y)\n",
        "    self.loss = loss\n",
        "    return loss\n",
        "\n",
        "  def backward_pass(self):\n",
        "\n",
        "    m = self.X.shape[0]\n",
        "\n",
        "    derv_Z3_wrt_W3 = self.l2.A\n",
        "    derv_loss_wrt_Z3 = self.l3.A - self.y\n",
        "    self.grad_W3 = 1/m * (np.dot(derv_Z3_wrt_W3.T, derv_loss_wrt_Z3))\n",
        "    self.grad_B3 = 1/m * (np.dot(np.ones((1, m)), derv_loss_wrt_Z3))\n",
        "\n",
        "    derv_Z2_wrt_W2 = self.l1.A\n",
        "    derv_A2_wrt_Z2 = (self.l2.Z > 0).astype('int')\n",
        "    derv_loss_wrt_A2 = np.dot(derv_loss_wrt_Z3, self.l3.W.T)\n",
        "\n",
        "    self.grad_W2 = 1/m * (np.dot(derv_Z2_wrt_W2.T, derv_loss_wrt_A2 * derv_A2_wrt_Z2))\n",
        "    self.grad_B2 = 1/m * (np.dot(np.ones((1, m)), derv_loss_wrt_A2 * derv_A2_wrt_Z2)) \n",
        "\n",
        "    derv_Z1_wrt_W1 = self.l1.X\n",
        "    derv_A1_wrt_Z1 = (self.l1.Z > 0).astype('int')\n",
        "    derv_loss_wrt_A1 = np.dot(derv_loss_wrt_A2 * derv_A2_wrt_Z2 , self.l2.W.T)\n",
        "\n",
        "    self.grad_W1 = 1/m * (np.dot(derv_Z1_wrt_W1.T, derv_loss_wrt_A1 * derv_A1_wrt_Z1))\n",
        "    self.grad_B1 = 1/m * (np.dot(np.ones((1, m)), derv_loss_wrt_A1 * derv_A1_wrt_Z1))\n",
        "    \n",
        "  def update_weights_bias(self):\n",
        "    self.l1.W -= self.learning_rate * self.grad_W1\n",
        "    self.l1.B -= self.learning_rate * self.grad_B1\n",
        "    self.l2.W -= self.learning_rate * self.grad_W2\n",
        "    self.l2.B -= self.learning_rate * self.grad_B2\n",
        "    self.l3.W -= self.learning_rate * self.grad_W3\n",
        "    self.l3.B -= self.learning_rate * self.grad_B3\n",
        "\n",
        "  def calculate_gradient_velocities(self):\n",
        "    self.l1.s_W = self.rho * self.l1.s_W + (1-self.rho) * (self.grad_W1**2)\n",
        "    self.l2.s_W = self.rho * self.l2.s_W + (1-self.rho) * (self.grad_W2**2)\n",
        "    self.l3.s_W = self.rho * self.l3.s_W + (1-self.rho) * (self.grad_W3**2)\n",
        "    \n",
        "    self.l1.s_B = self.rho * self.l1.s_B + (1-self.rho) * (self.grad_B1**2)\n",
        "    self.l2.s_B = self.rho * self.l2.s_B + (1-self.rho) * (self.grad_B2**2)\n",
        "    self.l3.s_B = self.rho * self.l3.s_B + (1-self.rho) * (self.grad_B3**2)\n",
        "\n",
        "  def update_weights_bias_RMSProp(self):\n",
        "    epsilon = 0.00000001\n",
        "\n",
        "    self.calculate_gradient_velocities()\n",
        "    self.l1.W -= self.learning_rate * (self.grad_W1/(np.sqrt(self.l1.s_W)+epsilon))\n",
        "    self.l1.B -= self.learning_rate * (self.grad_B1/(np.sqrt(self.l1.s_B)+epsilon))\n",
        "    self.l2.W -= self.learning_rate * (self.grad_W2/(np.sqrt(self.l2.s_W)+epsilon))\n",
        "    self.l2.B -= self.learning_rate * (self.grad_B2/(np.sqrt(self.l2.s_B)+epsilon))\n",
        "    self.l3.W -= self.learning_rate * (self.grad_W3/(np.sqrt(self.l3.s_W)+epsilon))\n",
        "    self.l3.B -= self.learning_rate * (self.grad_B3/(np.sqrt(self.l3.s_B)+epsilon))\n",
        "    pass\n",
        "\n",
        "\n",
        "  def fit(self, X, y, batch_size):\n",
        "    X, y = shuffle(X, y, random_state=0)\n",
        "    for epoch in range(self.epochs):\n",
        "      loss = 0\n",
        "      correct = 0\n",
        "      for i in range(0, X.shape[0], batch_size):\n",
        "        self.X = X[i:i + batch_size] # mini batch\n",
        "        self.y = y[i:i + batch_size]\n",
        "        self.forward_pass()\n",
        "        loss_epoch =  self.calculate_loss()\n",
        "        loss += loss_epoch\n",
        "        correct += self.correct_predictions()\n",
        "        self.backward_pass()\n",
        "        #self.update_weights_bias() # Gradient Descent\n",
        "        self.update_weights_bias_RMSProp() # RMSProp\n",
        "      print('Epoch', epoch, '======= Loss', loss, '======== Accuracy', \\\n",
        "            round((correct/X.shape[0])*100,2))\n",
        "    print('\\nAccuracy on Train set after',self.epochs,'epochs :',round((correct/X.shape[0])*100,2))\n",
        "    pass\n",
        "   \n",
        "  def evaluate(self, X_test, y_test):\n",
        "    self.forward_pass(X_test)\n",
        "    correct = self.correct_predictions(y_test)\n",
        "    return correct\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zchsMs_6pSN4",
        "colab_type": "text"
      },
      "source": [
        "**Activation Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhJNE2WYPcXe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## RELU\n",
        "f = lambda x: x if x > 0 else 0\n",
        "ReLU = np.vectorize(f)\n",
        "\n",
        "## Softmax\n",
        "def softmax_helper(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    epsilon = 0.000000001\n",
        "    e_x = np.exp(x - np.max(x)) + epsilon\n",
        "    # e_x = np.exp(x / x.max())\n",
        "    return e_x / (e_x.sum())\n",
        "def softmax(x):\n",
        "    return np.array(list(map(softmax_helper,x)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DH3bgJyPuE2O"
      },
      "source": [
        "Problem #1.2 (10 points): Train your fully-connected neural network on the Fashion-MNIST dataset using 5-fold cross validation. Report accuracy on the folds, as well as on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdbx0NVv8K52",
        "colab_type": "text"
      },
      "source": [
        "**Load the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XsN4sUoUugl8",
        "outputId": "7d64d756-b929-4f76-9fc8-fdf22f3cd3a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "source": [
        "# To simplify the usage of our dataset, we will be importing it from the Keras \n",
        "# library. Keras can be installed using pip: python -m pip install keras\n",
        "\n",
        "# Original source for the dataset:\n",
        "# https://github.com/zalandoresearch/fashion-mnist\n",
        "\n",
        "# Reference to the Fashion-MNIST's Keras function: \n",
        "# https://keras.io/datasets/#fashion-mnist-database-of-fashion-articles\n",
        "\n",
        "import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\n",
        "\n",
        "x_train = X_train.reshape(60000, 784)\n",
        "x_test = X_test.reshape(10000, 784)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "num_classes = 10\n",
        "y_train = keras.utils.to_categorical(Y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(Y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 1us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 1s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfm8s1iP8Xem",
        "colab_type": "text"
      },
      "source": [
        "**Visualize the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjTVUMU97iae",
        "colab_type": "code",
        "outputId": "55037e94-00e0-47ba-b4f3-573bc0e88e4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        }
      },
      "source": [
        "n_images = 20\n",
        "p = np.random.permutation(len(X_train))[:n_images]\n",
        "\n",
        "random_images = X_train[p]\n",
        "random_labels = Y_train[p]\n",
        "fig = plt.figure(figsize=(4,4))\n",
        "for i in range(n_images):\n",
        "  fig.add_subplot(n_images//5,5,i+1)\n",
        "  plt.axis('off')\n",
        "  plt.imshow(random_images[i], cmap = 'gray')\n",
        "  plt.title(random_labels[i])\n",
        "plt.show"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAADvCAYAAAAehVOEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 288x288 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60UVVlLj8buk",
        "colab_type": "text"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9GlB3Ql6ycP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 20\n",
        "lr = 0.001\n",
        "drop_out = 0.2\n",
        "rho = 0.9\n",
        "batch_size = 200\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGppVJovat5I",
        "colab_type": "text"
      },
      "source": [
        "**K-Fold Cross Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTdNXxn--ifQ",
        "colab_type": "code",
        "outputId": "b452bc9a-25d9-4d4b-e8a6-e7739e45bf82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "X = x_train\n",
        "y = y_train\n",
        "\n",
        "kf = KFold(n_splits=5)\n",
        "fold = 1\n",
        "accuracies = []\n",
        "for train_index, val_index in kf.split(X):\n",
        "  print(\"\\n### Fold:\", fold,'###')\n",
        "  X_tr, X_val = X[train_index], X[val_index]\n",
        "  y_tr, y_val = y[train_index], y[val_index]\n",
        "  fcn = NeuralNetwork(epochs, lr, drop_out, rho)\n",
        "  fcn.fit(X_tr, y_tr, batch_size)\n",
        "  correct = fcn.evaluate(X_val, y_val)\n",
        "  accuracies.append(round((correct/X_val.shape[0])*100,2))\n",
        "  print('Accuracy on hold-out Validation set:', accuracies[fold-1])\n",
        "  fold += 1\n",
        "print('\\nAverage validation accuracy after 5 fold cross-validation:', np.mean(accuracies))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "### Fold: 1 ###\n",
            "Epoch 0 ======= Loss 524.7462766998129 ======== Accuracy 51.29\n",
            "Epoch 1 ======= Loss 152.47035106338532 ======== Accuracy 78.46\n",
            "Epoch 2 ======= Loss 127.47332022859493 ======== Accuracy 81.98\n",
            "Epoch 3 ======= Loss 115.4933423040421 ======== Accuracy 83.68\n",
            "Epoch 4 ======= Loss 107.60059118413875 ======== Accuracy 84.67\n",
            "Epoch 5 ======= Loss 99.54101054841001 ======== Accuracy 85.71\n",
            "Epoch 6 ======= Loss 91.63174540861101 ======== Accuracy 86.65\n",
            "Epoch 7 ======= Loss 87.57772255955157 ======== Accuracy 87.17\n",
            "Epoch 8 ======= Loss 82.15129502428638 ======== Accuracy 88.03\n",
            "Epoch 9 ======= Loss 78.98318274765316 ======== Accuracy 88.41\n",
            "Epoch 10 ======= Loss 76.57402204043518 ======== Accuracy 88.69\n",
            "Epoch 11 ======= Loss 73.3755604961847 ======== Accuracy 89.16\n",
            "Epoch 12 ======= Loss 70.88745970430348 ======== Accuracy 89.4\n",
            "Epoch 13 ======= Loss 68.82344553038679 ======== Accuracy 89.81\n",
            "Epoch 14 ======= Loss 66.16226421234316 ======== Accuracy 90.08\n",
            "Epoch 15 ======= Loss 65.21149572643252 ======== Accuracy 90.18\n",
            "Epoch 16 ======= Loss 62.41482431195845 ======== Accuracy 90.67\n",
            "Epoch 17 ======= Loss 61.739906109345654 ======== Accuracy 90.7\n",
            "Epoch 18 ======= Loss 60.84290658303365 ======== Accuracy 90.81\n",
            "Epoch 19 ======= Loss 59.86509212349062 ======== Accuracy 91.03\n",
            "\n",
            "Accuracy on Train set after 20 epochs : 91.03\n",
            "Accuracy on hold-out Validation set: 87.21\n",
            "\n",
            "### Fold: 2 ###\n",
            "Epoch 0 ======= Loss 519.380270841278 ======== Accuracy 55.15\n",
            "Epoch 1 ======= Loss 149.6230159974063 ======== Accuracy 78.98\n",
            "Epoch 2 ======= Loss 131.94165943462363 ======== Accuracy 81.95\n",
            "Epoch 3 ======= Loss 118.27972899305621 ======== Accuracy 83.44\n",
            "Epoch 4 ======= Loss 105.60746882673654 ======== Accuracy 85.05\n",
            "Epoch 5 ======= Loss 96.81497626627528 ======== Accuracy 85.89\n",
            "Epoch 6 ======= Loss 90.691219195821 ======== Accuracy 86.85\n",
            "Epoch 7 ======= Loss 85.7428888476734 ======== Accuracy 87.51\n",
            "Epoch 8 ======= Loss 82.05944636963139 ======== Accuracy 87.89\n",
            "Epoch 9 ======= Loss 76.91996234991312 ======== Accuracy 88.6\n",
            "Epoch 10 ======= Loss 74.7995177235081 ======== Accuracy 88.92\n",
            "Epoch 11 ======= Loss 72.35001890232391 ======== Accuracy 89.25\n",
            "Epoch 12 ======= Loss 69.56926051766536 ======== Accuracy 89.56\n",
            "Epoch 13 ======= Loss 67.4245926674436 ======== Accuracy 89.84\n",
            "Epoch 14 ======= Loss 66.27193907862171 ======== Accuracy 90.12\n",
            "Epoch 15 ======= Loss 63.658064416338306 ======== Accuracy 90.55\n",
            "Epoch 16 ======= Loss 62.533230997877695 ======== Accuracy 90.65\n",
            "Epoch 17 ======= Loss 61.44811521036054 ======== Accuracy 90.7\n",
            "Epoch 18 ======= Loss 60.822446050247486 ======== Accuracy 90.88\n",
            "Epoch 19 ======= Loss 59.49195149544105 ======== Accuracy 91.2\n",
            "\n",
            "Accuracy on Train set after 20 epochs : 91.2\n",
            "Accuracy on hold-out Validation set: 87.85\n",
            "\n",
            "### Fold: 3 ###\n",
            "Epoch 0 ======= Loss 509.58777363705957 ======== Accuracy 52.95\n",
            "Epoch 1 ======= Loss 159.2385472523021 ======== Accuracy 77.89\n",
            "Epoch 2 ======= Loss 132.71750196912402 ======== Accuracy 81.55\n",
            "Epoch 3 ======= Loss 116.50361475309488 ======== Accuracy 83.62\n",
            "Epoch 4 ======= Loss 105.56524398824858 ======== Accuracy 85.04\n",
            "Epoch 5 ======= Loss 98.05986233005927 ======== Accuracy 85.86\n",
            "Epoch 6 ======= Loss 91.50095248064648 ======== Accuracy 86.69\n",
            "Epoch 7 ======= Loss 85.8882160929675 ======== Accuracy 87.5\n",
            "Epoch 8 ======= Loss 83.74254890174231 ======== Accuracy 87.68\n",
            "Epoch 9 ======= Loss 78.76027284805308 ======== Accuracy 88.44\n",
            "Epoch 10 ======= Loss 76.21203903344993 ======== Accuracy 88.8\n",
            "Epoch 11 ======= Loss 72.76303750028073 ======== Accuracy 89.22\n",
            "Epoch 12 ======= Loss 71.36212449590737 ======== Accuracy 89.41\n",
            "Epoch 13 ======= Loss 68.21528836936082 ======== Accuracy 89.76\n",
            "Epoch 14 ======= Loss 66.77457419484996 ======== Accuracy 89.78\n",
            "Epoch 15 ======= Loss 65.34488362743278 ======== Accuracy 90.24\n",
            "Epoch 16 ======= Loss 63.90366469311039 ======== Accuracy 90.36\n",
            "Epoch 17 ======= Loss 61.864030267632586 ======== Accuracy 90.69\n",
            "Epoch 18 ======= Loss 61.62340637013629 ======== Accuracy 90.62\n",
            "Epoch 19 ======= Loss 59.68417153476441 ======== Accuracy 90.89\n",
            "\n",
            "Accuracy on Train set after 20 epochs : 90.89\n",
            "Accuracy on hold-out Validation set: 87.88\n",
            "\n",
            "### Fold: 4 ###\n",
            "Epoch 0 ======= Loss 509.3600411756687 ======== Accuracy 51.27\n",
            "Epoch 1 ======= Loss 151.39787073064232 ======== Accuracy 78.49\n",
            "Epoch 2 ======= Loss 124.46799382957266 ======== Accuracy 82.11\n",
            "Epoch 3 ======= Loss 110.52556296445566 ======== Accuracy 83.99\n",
            "Epoch 4 ======= Loss 103.50812794368754 ======== Accuracy 85.2\n",
            "Epoch 5 ======= Loss 96.80526221238432 ======== Accuracy 85.9\n",
            "Epoch 6 ======= Loss 90.99846102222828 ======== Accuracy 86.61\n",
            "Epoch 7 ======= Loss 85.89185759858817 ======== Accuracy 87.39\n",
            "Epoch 8 ======= Loss 81.62926949035885 ======== Accuracy 87.73\n",
            "Epoch 9 ======= Loss 76.9670839943708 ======== Accuracy 88.52\n",
            "Epoch 10 ======= Loss 74.99057995877915 ======== Accuracy 88.74\n",
            "Epoch 11 ======= Loss 72.0136199482566 ======== Accuracy 89.18\n",
            "Epoch 12 ======= Loss 70.1589634385767 ======== Accuracy 89.44\n",
            "Epoch 13 ======= Loss 67.7389259740991 ======== Accuracy 89.81\n",
            "Epoch 14 ======= Loss 66.30209907860862 ======== Accuracy 90.09\n",
            "Epoch 15 ======= Loss 64.15282094951644 ======== Accuracy 90.2\n",
            "Epoch 16 ======= Loss 62.89602260694351 ======== Accuracy 90.47\n",
            "Epoch 17 ======= Loss 61.72156959152726 ======== Accuracy 90.54\n",
            "Epoch 18 ======= Loss 60.483615830091686 ======== Accuracy 90.68\n",
            "Epoch 19 ======= Loss 60.26761472856089 ======== Accuracy 90.88\n",
            "\n",
            "Accuracy on Train set after 20 epochs : 90.88\n",
            "Accuracy on hold-out Validation set: 87.27\n",
            "\n",
            "### Fold: 5 ###\n",
            "Epoch 0 ======= Loss 485.7642685078425 ======== Accuracy 54.0\n",
            "Epoch 1 ======= Loss 149.46100602320612 ======== Accuracy 78.91\n",
            "Epoch 2 ======= Loss 126.65002517817705 ======== Accuracy 82.24\n",
            "Epoch 3 ======= Loss 111.65839583802625 ======== Accuracy 83.95\n",
            "Epoch 4 ======= Loss 102.54761256262006 ======== Accuracy 85.31\n",
            "Epoch 5 ======= Loss 94.59554911627691 ======== Accuracy 86.32\n",
            "Epoch 6 ======= Loss 88.9821140908136 ======== Accuracy 87.05\n",
            "Epoch 7 ======= Loss 84.81479966380509 ======== Accuracy 87.62\n",
            "Epoch 8 ======= Loss 80.33124478746392 ======== Accuracy 88.21\n",
            "Epoch 9 ======= Loss 77.23061052669108 ======== Accuracy 88.64\n",
            "Epoch 10 ======= Loss 74.5159579165184 ======== Accuracy 88.91\n",
            "Epoch 11 ======= Loss 70.95353471331498 ======== Accuracy 89.41\n",
            "Epoch 12 ======= Loss 68.659368215513 ======== Accuracy 89.71\n",
            "Epoch 13 ======= Loss 67.17477754884551 ======== Accuracy 90.0\n",
            "Epoch 14 ======= Loss 65.55779167759182 ======== Accuracy 90.16\n",
            "Epoch 15 ======= Loss 63.51120510945781 ======== Accuracy 90.59\n",
            "Epoch 16 ======= Loss 61.58852541534932 ======== Accuracy 90.69\n",
            "Epoch 17 ======= Loss 60.34983301911251 ======== Accuracy 91.0\n",
            "Epoch 18 ======= Loss 58.83791243693048 ======== Accuracy 90.96\n",
            "Epoch 19 ======= Loss 57.966695181508854 ======== Accuracy 91.25\n",
            "\n",
            "Accuracy on Train set after 20 epochs : 91.25\n",
            "Accuracy on hold-out Validation set: 85.74\n",
            "\n",
            "Average validation accuracy after 5 fold cross-validation: 87.19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQq8cesI8hVf",
        "colab_type": "text"
      },
      "source": [
        "**Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3M0rkvR18i0O",
        "colab_type": "code",
        "outputId": "0b6b2528-8e25-4d99-8d60-177a8642c180",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "\n",
        "fcn = NeuralNetwork(epochs, lr, drop_out, rho)\n",
        "fcn.fit(x_train, y_train, batch_size)\n",
        "\n",
        "correct_predictions = fcn.evaluate(x_test, y_test)\n",
        "print('Accuracy on Test set:', round((correct_predictions/x_test.shape[0])*100,2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 ======= Loss 579.6430536704257 ======== Accuracy 58.67\n",
            "Epoch 1 ======= Loss 178.2550213613966 ======== Accuracy 80.03\n",
            "Epoch 2 ======= Loss 145.8154195814356 ======== Accuracy 83.22\n",
            "Epoch 3 ======= Loss 127.98572769091096 ======== Accuracy 85.04\n",
            "Epoch 4 ======= Loss 115.78228541338511 ======== Accuracy 86.31\n",
            "Epoch 5 ======= Loss 109.40091862490036 ======== Accuracy 87.11\n",
            "Epoch 6 ======= Loss 104.48706202625362 ======== Accuracy 87.68\n",
            "Epoch 7 ======= Loss 98.7664171816656 ======== Accuracy 88.29\n",
            "Epoch 8 ======= Loss 94.7036485056005 ======== Accuracy 88.63\n",
            "Epoch 9 ======= Loss 90.68832634112549 ======== Accuracy 89.12\n",
            "Epoch 10 ======= Loss 89.00911958423737 ======== Accuracy 89.32\n",
            "Epoch 11 ======= Loss 86.16037332143165 ======== Accuracy 89.58\n",
            "Epoch 12 ======= Loss 83.39928174329489 ======== Accuracy 89.9\n",
            "Epoch 13 ======= Loss 81.38024866953708 ======== Accuracy 90.08\n",
            "Epoch 14 ======= Loss 80.35091281264904 ======== Accuracy 90.17\n",
            "Epoch 15 ======= Loss 78.91184958396755 ======== Accuracy 90.46\n",
            "Epoch 16 ======= Loss 79.81900666226856 ======== Accuracy 90.53\n",
            "Epoch 17 ======= Loss 77.53948841249097 ======== Accuracy 90.56\n",
            "Epoch 18 ======= Loss 77.95123594998209 ======== Accuracy 90.49\n",
            "Epoch 19 ======= Loss 77.53509306513169 ======== Accuracy 90.81\n",
            "\n",
            "Accuracy on Train set after 20 epochs : 90.81\n",
            "Accuracy on Test set: 86.07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NsYWLhVUt_In"
      },
      "source": [
        "Problem #2.1 (40 points): Implement a Convolutional Neural Network from scratch. Similarly to problem 1.1, we will be implementing the same architecture as the one shown in [Keras' CNN documentation](https://keras.io/examples/mnist_cnn/). That is:\n",
        "\n",
        "- Input layer\n",
        "- Convolutional hidden layer with 32 neurons, a kernel size of (3,3), and relu activation function\n",
        "- Convolutional hidden layer with 64 neurons, a kernel size of (3,3), and relu activation function\n",
        "- Maxpooling with a pool size of (2,2)\n",
        "- Dropout with a value of 0.25\n",
        "- Flatten layer\n",
        "- Dense hidden layer, with 128 neurons, and relu activation function\n",
        "- Dropout with a value of 0.5\n",
        "- Output layer, using softmax as the activation function\n",
        "\n",
        "Our loss function is categorical crossentropy and the evaluation will be done using accuracy, as in Problem 1.1. However, we will not be using the gradient optimizer known as Adadelta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wEGyV8qTvX7H",
        "colab": {}
      },
      "source": [
        "class ConvolutionalNeuralNetwork(object):\n",
        "  def __init__(epochs, learning_rate):\n",
        "    pass\n",
        "  \n",
        "  def fit(self):\n",
        "    pass\n",
        "  \n",
        "  def evaluate(self):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nmek2iUovYEk"
      },
      "source": [
        "Problem #2.2 (10 points): Train your convolutional neural network on the Fashion-MNIST dataset using 5-fold cross validation. Report accuracy on the folds, as well as on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B6VsoxNSwFVH",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "num_classes = 10\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}